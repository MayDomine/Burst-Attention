{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "@dataclass\n",
    "class Exp:\n",
    "    name:str\n",
    "    hidden_size:int\n",
    "    batch_size:int\n",
    "    seqlen:int\n",
    "    num_head:int\n",
    "    multi_gpu:bool = \"True\"\n",
    "    elapsed_time:str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "def record_gpu_memory_usage(output_file_path):\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        process = subprocess.Popen(['nvidia-smi', '--query-gpu=memory.used', '--format=csv',\"-l\"],\n",
    "                                   stdout=subprocess.PIPE,\n",
    "                                   stderr=subprocess.PIPE)\n",
    "        \n",
    "        p = psutil.Process(process.pid)\n",
    "        while True:\n",
    "            output, error = process.communicate()\n",
    "            output = output.decode('utf-8').strip()\n",
    "            error = error.decode('utf-8').strip()\n",
    "            if error:\n",
    "                print(f\"Error while recording GPU memory usage: {error}\")\n",
    "                break\n",
    "            f.write(f\"{output}\\n\")\n",
    "            time.sleep(1)\n",
    "            if not p.is_running():\n",
    "                break\n",
    "        process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"burst_attn_simple.py\", line 205, in <module>\n",
      "    test_multi_gpu(batch_size,hidden_size,seqlen,num_heads,func,args.desc,args.backward)\n",
      "  File \"burst_attn_simple.py\", line 105, in test_multi_gpu\n",
      "    func(q_whole,k_whole,v_whole,backward)\n",
      "  File \"burst_attn_simple.py\", line 121, in test_ref\n",
      "    torch.autograd.grad(res_ref, (q, k, v), g)\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 276, in grad\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 2; 39.59 GiB total capacity; 24.75 GiB already allocated; 5.78 GiB free; 32.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"burst_attn_simple.py\", line 205, in <module>\n",
      "    test_multi_gpu(batch_size,hidden_size,seqlen,num_heads,func,args.desc,args.backward)\n",
      "  File \"burst_attn_simple.py\", line 105, in test_multi_gpu\n",
      "    func(q_whole,k_whole,v_whole,backward)\n",
      "  File \"burst_attn_simple.py\", line 121, in test_ref\n",
      "    torch.autograd.grad(res_ref, (q, k, v), g)\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 276, in grad\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 3; 39.59 GiB total capacity; 24.75 GiB already allocated; 5.78 GiB free; 32.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"burst_attn_simple.py\", line 205, in <module>\n",
      "    test_multi_gpu(batch_size,hidden_size,seqlen,num_heads,func,args.desc,args.backward)\n",
      "  File \"burst_attn_simple.py\", line 105, in test_multi_gpu\n",
      "    func(q_whole,k_whole,v_whole,backward)\n",
      "  File \"burst_attn_simple.py\", line 121, in test_ref\n",
      "    torch.autograd.grad(res_ref, (q, k, v), g)\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 276, in grad\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 1; 39.59 GiB total capacity; 24.75 GiB already allocated; 5.78 GiB free; 32.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"burst_attn_simple.py\", line 205, in <module>\n",
      "    test_multi_gpu(batch_size,hidden_size,seqlen,num_heads,func,args.desc,args.backward)\n",
      "  File \"burst_attn_simple.py\", line 105, in test_multi_gpu\n",
      "    func(q_whole,k_whole,v_whole,backward)\n",
      "  File \"burst_attn_simple.py\", line 121, in test_ref\n",
      "    torch.autograd.grad(res_ref, (q, k, v), g)\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 276, in grad\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 39.59 GiB total capacity; 24.75 GiB already allocated; 5.78 GiB free; 32.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 46766) of binary: /root/data/final_env/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/data/final_env/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch', 'console_scripts', 'torchrun')())\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/distributed/run.py\", line 761, in main\n",
      "    run(args)\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/distributed/run.py\", line 752, in run\n",
      "    elastic_launch(\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/root/data/final_env/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 245, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "burst_attn_simple.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2023-04-07_11:07:00\n",
      "  host      : ubuntu\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 46767)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2023-04-07_11:07:00\n",
      "  host      : ubuntu\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 46768)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[3]:\n",
      "  time      : 2023-04-07_11:07:00\n",
      "  host      : ubuntu\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 46769)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-04-07_11:07:00\n",
      "  host      : ubuntu\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 46766)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run(['bash','./a.sh'], stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "\n",
    "output = result.stdout.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_exp_out(output, gpu_info_file):\n",
    "    with open(gpu_info_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        count = 0\n",
    "        for idx in range(len(lines)//4):\n",
    "            line = lines[idx]\n",
    "            if int(line.split()[0]) == 0 and count==0:\n",
    "                continue\n",
    "            elif int(line.split()[0] !=0 and count==0):\n",
    "                count+=1\n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "cmd = \"nvidia-smi --query-gpu=memory.used --format=csv -l 1\"\n",
    "res = subprocess.check_output(cmd,shell=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"png\"\n",
    "pic = ['forward_memory.png','forward_output.png','memory.png','output.png']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
